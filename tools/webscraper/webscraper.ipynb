{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libs.common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m html2md,url2md\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'libs.common'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "\n",
    "from libs.common import html2md, url2md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import requests\n",
    "\n",
    "# from googlesearch import search as google_search\n",
    "# search_string = \"Erik Hodges\"\n",
    "\n",
    "\n",
    "# def parse_url(url:str):\n",
    "#     url = url.replace('https://','')\n",
    "#     base = url.split('/')[0]\n",
    "#     path = url.replace(base,'')\n",
    "#     return base,path\n",
    "\n",
    "# def search_web(search_string:str,n_results:int=10):\n",
    "\n",
    "#     results = google_search(\n",
    "#         search_string,\n",
    "#         advanced=True,\n",
    "#         num_results=n_results,\n",
    "#         )\n",
    "\n",
    "#     result_list = []\n",
    "#     for res in results:\n",
    "#         result_list.append(res)\n",
    "\n",
    "#     df_search_results = []\n",
    "#     for idx,res in enumerate(result_list):\n",
    "#         base,path = parse_url(res.url)\n",
    "#         # print(f\"{base = }\")\n",
    "#         # print(f\"{path = }\")\n",
    "#         # response = requests.request('GET', res.url)\n",
    "#         search_results = {\n",
    "#             'url': res.url,\n",
    "#             'title': res.title[:50],\n",
    "#             'description':res.description[:100],\n",
    "#             # 'base': parse_url(res.url)[0],\n",
    "#             # 'path': parse_url(res.url)[1],\n",
    "#             # \"response\": requests.request('GET', res.url)\n",
    "#         }\n",
    "#         df_search_results.append(search_results)\n",
    "\n",
    "\n",
    "#     return df_search_results\n",
    "\n",
    "\n",
    "# df_search_results = search_web(search_string)\n",
    "\n",
    "# df_search_results\n",
    "\n",
    "\n",
    "# ########### USE THIS SCRIPT TO ACTUALLY PARSE THE PAGE - or at least try to...\n",
    "\n",
    "# # import requests\n",
    "# # from bs4 import BeautifulSoup\n",
    "# # import requests\n",
    "# # from fake_useragent import UserAgent\n",
    "# # ua = UserAgent(min_percentage=1.3)\n",
    "# # ua_num = ua.random\n",
    "\n",
    "# # # Making a get request\n",
    "\n",
    "# # test_url = \"https://www.wired2fish.com/bass-fishing/how-to-catch-bass-easy-bass-fishing-tips\"\n",
    "\n",
    "# # response = requests.get(test_url,headers={'user-agent':ua_num})\n",
    "# # soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# # page_text = soup.get_text().replace('\\n','').strip()\n",
    "# # print(page_text)\n",
    "\n",
    "\n",
    "# ################################# THIS IS AN EXAMPLE OF A BETTER FUNCTION TO SCRAPE THE WEB\n",
    "\n",
    "# # def scrape_website(state: AgentGraphState, research=None):\n",
    "# #     research_data = research().content\n",
    "# #     research_data = json.loads(research_data)\n",
    "# #     # research_data = ast.literal_eval(research_data)\n",
    "\n",
    "# #     try:\n",
    "# #         url = research_data[\"selected_page_url\"]\n",
    "# #     except KeyError as e:\n",
    "# #         url = research_data[\"error\"]\n",
    "\n",
    "# #     try:\n",
    "# #         response = requests.get(url)\n",
    "# #         response.raise_for_status()\n",
    "# #         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# #         # Extract text content\n",
    "# #         texts = soup.stripped_strings\n",
    "# #         content = ' '.join(texts)\n",
    "\n",
    "# #         # Check for garbled text\n",
    "# #         if is_garbled(content):\n",
    "# #             content = \"error in scraping website, garbled text returned\"\n",
    "# #         else:\n",
    "# #             # Limit the content to 4000 characters\n",
    "# #             content = content[:4000]\n",
    "\n",
    "# #         state[\"scraper_response\"].append(HumanMessage(role=\"system\", content=str({\"source\": url, \"content\": content})))\n",
    "\n",
    "# #         return {\"scraper_response\": state[\"scraper_response\"]}\n",
    "\n",
    "# #     except requests.HTTPError as e:\n",
    "# #         if e.response.status_code == 403:\n",
    "# #             content = f\"error in scraping website, 403 Forbidden for url: {url}\"\n",
    "# #         else:\n",
    "# #             content = f\"error in scraping website, {str(e)}\"\n",
    "\n",
    "# #         state[\"scraper_response\"].append(HumanMessage(role=\"system\", content=str({\"source\": url, \"content\": content})))\n",
    "# #         return {\"scraper_response\": state[\"scraper_response\"]}\n",
    "# #     except requests.RequestException as e:\n",
    "# #         content = f\"error in scraping website, {str(e)}\"\n",
    "# #         state[\"scraper_response\"].append(HumanMessage(role=\"system\", content=str({\"source\": url, \"content\": content})))\n",
    "# #         return {\"scraper_response\": state[\"scraper_response\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to write object type <class 'str'> to google_styleguide_wordlist.md\n",
      "google_styleguide_wordlist.md has been WRITTEN!\n"
     ]
    }
   ],
   "source": [
    "basename = \"google_styleguide_wordlist\"\n",
    "# url = 'https://google.github.io/styleguide/pyguide.html'\n",
    "url = \"https://developers.google.com/style/word-list\"\n",
    "\n",
    "url2md(url, basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function if you need to convert a previously saved html file to markdown\n",
    "html2md(\"M:/Code/ML/ELH/data/webpages/python_style_guide.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At some point I'll want to make it so that my url2md function can iterate over a list of pages\n",
    "\n",
    "url_base = \"https://google.github.io/styleguide/\"\n",
    "pages = [\n",
    "    \"pyguide\",\n",
    "]\n",
    "\n",
    "for page in pages:\n",
    "    url = f\"{url_base}{pages}.html\"\n",
    "    response = requests.request(\"GET\", url)\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
