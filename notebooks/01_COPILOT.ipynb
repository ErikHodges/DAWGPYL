{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAWGPYL IMPORTS\n",
    "from dawgpyl.libs.env import *\n",
    "from dawgpyl.configs.apis import *\n",
    "from dawgpyl.configs.core import *\n",
    "from dawgpyl.libs.common import *\n",
    "from dawgpyl.libs.base import *\n",
    "from dawgpyl.libs.io import *\n",
    "\n",
    "\n",
    "\n",
    "# from flask.views import F\n",
    "\n",
    "dirs = Directories()\n",
    "log_prefix = \"log_\"\n",
    "log_filename = log_prefix + Timestamp().date + \".joblib\"\n",
    "log_file_path = os.path.join(dirs.logs, log_filename)\n",
    "\n",
    "# print_heading(\"Available APIs and Models\",'green')\n",
    "# eprint(MODELS)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print_heading(\"Available Agent Personas\", \"green\")\n",
    "eprint([x for x in list(AGENTS.keys())])\n",
    "print(\"\\n\")\n",
    "\n",
    "print_heading(\"Project Directories\", \"green\")\n",
    "print_dict(dirs.__dict__, \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load daily response log and display last response\n",
    "log = read_log_file(log_file_path)\n",
    "# log_example = read_log_file(os.path.join(dirs.logs, \"log_2024-11-13.joblib\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_context(filepaths: list | str):\n",
    "\n",
    "    if filepaths:\n",
    "        if isinstance(filepaths, list):\n",
    "            prompt_input = \"CONTEXT\"\n",
    "            for f in filepaths:\n",
    "                prompt_input += \"\\n\" + str(\n",
    "                    read_file(f)\n",
    "                )  # This should only be done files that are read as string (.txt, .py, etc.)\n",
    "\n",
    "        else:\n",
    "            prompt_input = read_file(filepaths)\n",
    "    else:\n",
    "        prompt_input = \"None\"\n",
    "\n",
    "    return prompt_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General Q&A\n",
    "### Code Development\n",
    "save_response = True\n",
    "\n",
    "model_name = \"o1-preview\"\n",
    "max_tokens = 10_000\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "prompt_files = []\n",
    "\n",
    "prompt_input = load_prompt_context(prompt_files)\n",
    "\n",
    "prompt_system = \"\"\"\n",
    "You are an expert personal assistant. You provide advice and guidance to help your user make decisions.\n",
    "Your preferred output method is markdown. You are bound by fiduciary duty to provide accurate and relevant information.\n",
    "\"\"\"\n",
    "\n",
    "prompt_user = f\"\"\"\n",
    "Please create an elaborate menu that I can prepare for them and provide a shopping list.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "chat_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt_system\": prompt_system,\n",
    "    \"prompt_user\": prompt_user,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "### Invoke Response\n",
    "response = chat(**chat_config)\n",
    "\n",
    "### Display Response\n",
    "try:\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "except:\n",
    "    try:\n",
    "        eprint(response.choices[0].message.content)\n",
    "    except:\n",
    "        try:\n",
    "            print_dict(response.choices[0].message.content)\n",
    "        except:\n",
    "            print(response)\n",
    "\n",
    "\n",
    "### Save Response\n",
    "if save_response:\n",
    "    log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Task Breakdown\n",
    "\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "# prompt_system = \"\"\"\n",
    "# You are a skilled research assistant.\n",
    "# You provide succinct yet comprehensive responses to each user request.\n",
    "# You retrieve information from reliable and trustworthy sources. You cite these sources when appropriate.\n",
    "# Before responding, you consider the logical steps needed to provide your response and articulate them.\n",
    "# Your final response should be presented in markdown format.\n",
    "# \"\"\"\n",
    "# prompt_user = \"\"\"\n",
    "# I would like help breaking down complex tasks into more manageable steps. Please use the web to research methods for breaking down complex tasks into sub-tasks, then provide a summary of your findings.\n",
    "# \"\"\"\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Writing w/ Context Files\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "\n",
    "# ###############################################################################################################\n",
    "# ### Use this section if you want one file read for the system prompt and one file read for the user prompt\n",
    "# filepaths = [\n",
    "#     \"data/knowledge/file_00.md\",\n",
    "#     ]\n",
    "\n",
    "\n",
    "# prompt_input = load_prompt_files(filepaths[0])\n",
    "# prompt_system = f\"\"\"\n",
    "# You are a skilled scientific writer.\n",
    "# You excel at writing about complex topics with clarity and accuracy.\n",
    "# Your writing style is similar to other highly-cited researcher who publish in top academic journals (Nature, Science, New England Journal of Medicine, etc.)\n",
    "# You retrieve information from reliable and trustworthy sources. You cite these sources when appropriate.\n",
    "# Before responding, you consider the logical steps needed to provide your response and articulate them step-by-step.\n",
    "\n",
    "# The following information is the context for your writing assignment.\\n\n",
    "\n",
    "# CONTEXT:\\n\\n\n",
    "# {prompt_input}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# #Please review the following text and provide editorial guidance to improve the clarity and quality of the text.\n",
    "# #Please review the following text and provide 2-3 additional sentences to each section which expand and elaborate on the current topics.\n",
    "# prompt_input = load_prompt_files(\"data/document_00.txt\")\n",
    "\n",
    "# prompt_user = f\"\"\"\n",
    "# Please review the following text and edit sections to match the factual information contained in the CONTEXT. Additionally, use your editorial expertise to improve the scientific rigor, clarity, and quality of the text.\n",
    "\n",
    "# CURRENT_DRAFT:\\n\\n\n",
    "\n",
    "# {prompt_input}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code Documentation (inline)\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "\n",
    "# ###############################################################################################################\n",
    "# filepath = \"code/code2clean_00.py\"\n",
    "# prompt_input = read_file(filepath)\n",
    "# prompt_system = \"\"\"You are an expert software engineer who writes detailed documentation\n",
    "#             for all of your code. Your preferred code documentation standard is the\n",
    "#             \"Google Python Style Guide\". Each argument of a function should contain\n",
    "#             a datatype within the definition line, and immediately under each\n",
    "#             function definition there should be a comment block describing the\n",
    "#             function's arguments and return values. If a function contains more than a few lines, you should\n",
    "#             also provide contextual comments inline with the code.\"\"\"\n",
    "\n",
    "# prompt_user = f\"\"\"\n",
    "\n",
    "#             Please review the following code and document it appropriately.\n",
    "\n",
    "#             'EXAMPLE CODE'\\n\\n\n",
    "#             {prompt_input}\\n\\n\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Development\n",
    "save_response = True\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "max_tokens = None\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "prompt_files = []\n",
    "\n",
    "prompt_input = load_prompt_files(prompt_files)\n",
    "\n",
    "prompt_system = \"\"\"\n",
    "            You are an expert software engineer who writes efficient python code with \n",
    "            detailed docstrings and inline documentation for all of your code. \n",
    "            Your preferred code documentation standard is the \"Google Python Style Guide\".\n",
    "            Each argument of a function should contain a datatype within the definition line, \n",
    "            and immediately under each function definition there should be a comment block \n",
    "            describing the function's arguments and return values. \n",
    "            If a function contains more than a few lines, you should also provide contextual comments \n",
    "            inline with the code.\"\"\"\n",
    "\n",
    "prompt_user = f\"\"\"\n",
    "    I would like you to write a library of python functions that reads various filetypes, \n",
    "    describes their contents, and ensures that each file's contents satisfy a series of data quality checks. \n",
    "    \n",
    "    This library should be able to parse the following filetypes: ['.hdf5','.parquet','.avi','.pdf']\n",
    "\n",
    "    The descriptions of each files contents should include: ['file_size_in_megabytes','file_length','contains_datetimes','sample_rate']\n",
    "\n",
    "    The filetype-specific data quality checks should include aspects of the data such as: \n",
    "    ['readable','has_missing_values']\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt_system\": prompt_system,\n",
    "    \"prompt_user\": prompt_user,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "### Invoke Response\n",
    "response = chat(**chat_config)\n",
    "\n",
    "### Display Response\n",
    "try:\n",
    "    display(md(response.choices[0].message.content))\n",
    "except:\n",
    "    try:\n",
    "        print(response.choices[0].message.content)\n",
    "    except:\n",
    "        print(response)\n",
    "\n",
    "\n",
    "### Save Response\n",
    "if save_response:\n",
    "    log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it seems the conclusion is I need to be using HDF5 files, but joblib is WAAAAY easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_log_path = \"data/logs/LOG2EDIT.joblib\"\n",
    "edited_log = read_file(edited_log_path)\n",
    "\n",
    "### In case you need to manually edit a chat log\n",
    "# from openai.types.chat import ChatCompletion,ChatCompletionMessage\n",
    "# from openai.types.chat.chat_completion import Choice\n",
    "# from openai.types.completion_usage import CompletionUsage\n",
    "\n",
    "# write_file(edited_log, edited_log_path)\n",
    "# write_file(log, os.path.join(dirs.logs,'LOG2EDIT.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Simple Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS A SIMPLE OPENAI CHAT WRAPPER, USE IT TO EXAMINE RAW MODEL OUTPUT\n",
    "\n",
    "from libs.common import chat\n",
    "\n",
    "max_tokens = 2048\n",
    "model_name = \"gpt-4o\"\n",
    "prompt_system = \"You are a helpful assistant.\"\n",
    "prompt_user = \"Tell me a story about a fire department run by bunnies.\"\n",
    "\n",
    "response = chat(model_name, prompt_system, prompt_user, max_tokens)\n",
    "\n",
    "response_dict = response.__dict__\n",
    "print(response_dict[\"choices\"][0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
