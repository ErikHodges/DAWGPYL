{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.env import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from configs.apis import APIS\n",
    "\n",
    "from libs.agents import Agent, AgentConfig, create_agent, invoke_agent\n",
    "from libs.base import Directories,Timestamp\n",
    "from libs.graphs import run_team_workflow\n",
    "from libs.io import read_text, write_text\n",
    "# from rich import print as richprint\n",
    "from IPython.display import Markdown as md\n",
    "# from flask.views import F\n",
    "\n",
    "dirs = Directories()\n",
    "log_prefix = \"log_\"\n",
    "filename = log_prefix + Timestamp().date + \".joblib\"\n",
    "log_file_path = os.path.join(dirs.logs, filename)\n",
    "\n",
    "# print_heading(\"Available APIs and Models\",'green')\n",
    "# eprint(MODELS)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print_heading(\"Available Agent Personas\", \"green\")\n",
    "eprint([x for x in list(AGENTS.keys())])\n",
    "print(\"\\n\")\n",
    "\n",
    "# print_heading(\"Project Directories\", \"green\")\n",
    "# print_dict(dirs.__dict__, \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load daily response log and display last response\n",
    "\n",
    "try:\n",
    "    log = read_file(log_file_path)\n",
    "    try:\n",
    "        display(md(log[-1]['response']))\n",
    "    except:\n",
    "        print(log)\n",
    "except:\n",
    "    print(\"DAILY LOG NOT FOUND\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'code/hub'\n",
    "\n",
    "# Use this for databricks?\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/dbfs/hugging_face_transformers_cache/'\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "hf_token = APIS[\"huggingface\"][\"key\"].get_secret_value()\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model_name, \n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "#     device_map=\"auto\",\n",
    "#     token=hf_token,\n",
    "#     )\n",
    "# pipeline(\"Hey how are you doing today?\")\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "os.environ['HF_HOME'] = 'M:/Code/resources/hub'\n",
    "\n",
    "\n",
    "model_name = \"google/gemma-2-9b\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=hf_token)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10,token=hf_token)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # \"Qwen/Qwen2-72B-Instruct\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('code/llama3')\n",
    "import fire\n",
    "from llama import Dialog, Llama\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "ckpt_dir = 'M:/Code/llama3/Meta-Llama-3-8B-Instruct/'\n",
    "tokenizer_path = 'M:/Code/llama3/Meta-Llama-3-8B-Instruct/tokenizer.model'\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 64\n",
    "\n",
    "# generator = Llama.build(\n",
    "#         ckpt_dir=ckpt_dir,\n",
    "#         tokenizer_path=tokenizer_path,\n",
    "#         max_seq_len=max_seq_len,\n",
    "#         max_batch_size=max_batch_size,\n",
    "#     )\n",
    "\n",
    "local_llm = llama3\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
