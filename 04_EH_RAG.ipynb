{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpLetJyopE7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from libs.agents import Agent, AgentConfig, create_agent, invoke_agent\n",
    "from libs.ai import *\n",
    "from libs.base import Directories\n",
    "from libs.graphs import run_team_workflow\n",
    "from libs.io import read_text, write_text\n",
    "from security.apis import APIS\n",
    "\n",
    "dirs = Directories()\n",
    "log_file_path = os.path.join(dirs.logs,'log.py')\n",
    "\n",
    "# print_heading(\"Available APIs and Models\",'green')\n",
    "# eprint(MODELS)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print_heading(\"Available Agent Personas\",'green')\n",
    "eprint([x for x in list(AGENTS.keys())])\n",
    "print(\"\\n\")\n",
    "\n",
    "print_heading(\"Project Directories\",'green')\n",
    "print_dict(dirs.__dict__,'green')\n",
    "\n",
    "\n",
    "\n",
    "# from openai import OpenAI\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_openai import OpenAI\n",
    "# import langgraph\n",
    "\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langgraph.graph import END, MessageGraph\n",
    "\n",
    "# from langchain.vectorstores import FAISS\n",
    "import chromadb\n",
    "# from chromadb import Client as VectorDBClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader,PyMuPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_root = \"ELH/code/configs/\"\n",
    "\n",
    "model_config = libs.io.file_open(f\"{config_root}models.json\")\n",
    "\n",
    "print(\"Available APIs and Models\")\n",
    "print(\"--------------------------\")\n",
    "# _=[print(x) for x in list(model_config.keys())]\n",
    "_=[print(f\"{x}:{model_config[x]['models']}\") for x in list(model_config.keys())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONFIGURE THE RAG SYSTEM\n",
    "\n",
    "api = 'nvidia' \n",
    "# api_key = model_config[api]['key']\n",
    "api_key = model_config.get(api).get('key')\n",
    "\n",
    "\n",
    "model_name = \"mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "# embedder_name = \"NV-Embed-QA\"\n",
    "embedder_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "dir_vector_db = \"data/vector_db\"\n",
    "dir_project = \"dawgpyl\"\n",
    "dir_filestore = f\"{dir_project}data/docs/\"\n",
    "DOCS_DIR = os.path.abspath(dir_filestore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run and see that you can generate a response successfully\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    " \n",
    "llm = ChatNVIDIA(model=model_name, nvidia_api_key=api_key, max_tokens=1024)\n",
    "\n",
    "result = llm.invoke(\"Can I run multiple docker containers on the NVIDIA API?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the vector embeddings in chromadb using the langchain wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vector database\n",
    "# create the open-source embedding function\n",
    "\n",
    "document_embedder = SentenceTransformerEmbeddings(model_name=embedder_name)\n",
    "\n",
    "text_splitter_config = {\n",
    "    \"chunk_size\":2500,\n",
    "    \"chunk_overlap\":250,\n",
    "    \"separator\":\" \",\n",
    "}\n",
    "text_splitter = CharacterTextSplitter(**text_splitter_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Spare attempts at creating a chromadb client\n",
    "\n",
    "# vector_db_client = chromadb.PersistentClient(path=vector_db_dir)\n",
    "# print(type(vector_db_client))\n",
    "# vector_db_client\n",
    "\n",
    "# chroma_client = VectorDBClient()\n",
    "# collection_name = 'science'\n",
    "# collection = chroma_client.create_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(dir_vector_db):    \n",
    "    ######## LOAD THE VECTOR DATABASE\n",
    "    print(\"VECTOR DB ALREADY EXISTS\")\n",
    "    print(f\"LOADING: {dir_vector_db}\")\n",
    "    vector_db = Chroma(persist_directory=dir_vector_db, embedding_function=document_embedder)\n",
    "\n",
    "else:\n",
    "    ####### CREATE THE VECTOR DATABASE IF IT DOESN'T ALREADY EXIST\n",
    "\n",
    "    # Load a pdf file\n",
    "    # document_filepath = \"data/Principles of Neural Science - Fifth Edition.pdf\"\n",
    "    knowledge_dir_files = [os.path.join(dir_filestore,x) for x in os.listdir(dir_filestore)]\n",
    "    for idx,document_filepath in enumerate(knowledge_dir_files):\n",
    "        # document_filepath = \"data\\docs\\American Psychiatric Association - Diagnostic and Statistical Manual of Mental Disorders, 5th Edition_ DSM-5 (2013, American Psychiatric Publishing).pdf\"\n",
    "        if '.pdf' in document_filepath:\n",
    "            document_loader = PyMuPDFLoader(document_filepath)\n",
    "            pages = document_loader.load()\n",
    "\n",
    "        # Split the text into chunks smallter than a maximum size    \n",
    "        pages_split = text_splitter.split_documents(pages)\n",
    "\n",
    "        # Embed the chunks and save in a Chroma vector database\n",
    "        if idx==0:\n",
    "            vector_db = Chroma.from_documents(pages_split, document_embedder, persist_directory=dir_vector_db)\n",
    "        else:\n",
    "            vector_db = Chroma.add_documents(pages_split, document_embedder, persist_directory=dir_vector_db)\n",
    "    # print(type(vector_db))\n",
    "    \n",
    "\n",
    "_=[print(x) for x in list(vector_db.get().keys())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the CharacterTextSplitter appears to only split lines that are greater than 400 characters in length.  <br>\n",
    "I'm not sure what the implications are for this, but that is okay for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apparently I re-wrote the 'text_splitter.split_documents' function... OH BOY\n",
    "\n",
    "# all_page_text = []\n",
    "# page_nums = []\n",
    "\n",
    "\n",
    "# for idx,page in enumerate(pages):\n",
    "#     page_dict = page.dict()\n",
    "#     page_text = [page_dict['page_content']]\n",
    "#     all_page_text.extend(page_text)\n",
    "#     page_nums.extend([str(page_dict['metadata']['page'])])\n",
    "\n",
    "# del pages\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0, separator=\" \")\n",
    "# pages_split = []\n",
    "# pages_split_metadata = []\n",
    "\n",
    "# for idx, page_text in enumerate(all_page_text):\n",
    "#     splits = text_splitter.split_text(page_text)\n",
    "#     pages_split.extend(splits)\n",
    "#     pages_split_metadata.extend([{\"page_num\": page_nums[idx]}] * len(splits))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## QUERY THE VECTOR DATABASE\n",
    "\n",
    "# query = \"Who is Donald Hebb?\"\n",
    "# query = \"What are some symptoms of borderline personality disorder?\"\n",
    "query = \"What is a machine learning model?\"\n",
    "# query_results = chroma_db.similarity_search(query)\n",
    "query_results = vector_db.similarity_search_with_score(query,k=5)\n",
    "\n",
    "\n",
    "print(f\"SEARCHING VECTOR DATABASE: {dir_vector_db}\")\n",
    "print()\n",
    "print(f\"QUERY: {query}\")\n",
    "print()\n",
    "for result,score in query_results:\n",
    "    result = result.dict()\n",
    "    result_meta = result['metadata']\n",
    "\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    print(f\"SIMILARITY SCORE: {score}\")\n",
    "    print(f\"Source: {result_meta['source']}\")\n",
    "    print(f\"Page: {result_meta['page']}\")\n",
    "    print()\n",
    "    print(result['page_content'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **END RAG**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQJaXhDn6y64L9cluYrFJb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
