{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "## CORE IMPORTS\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Annotated, List, TypedDict\n",
    "from uuid import uuid4\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyspark\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# EH Custom\n",
    "# import libs.ai as ail\n",
    "from configs.agents import AGENTS\n",
    "from configs.models import MODELS\n",
    "from configs.prompts import PROMPTS\n",
    "import libs.agent_tools as atools\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "os.chdir(\"dawgpyl/code\")\n",
    "print(os.getcwd())\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "## Imports for Development\n",
    "\n",
    "### Web Retrieval\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### Data Storage\n",
    "import chromadb\n",
    "# from chromadb import Client as VectorDBClient\n",
    "\n",
    "\n",
    "## LLML\n",
    "### LangChain I/O\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import (\n",
    "    PDFMinerLoader,\n",
    "    PDFPlumberLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "### LangChain Inference\n",
    "from langchain_core.messages import AIMessage, AnyMessage, ChatMessage, HumanMessage, SystemMessage\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "### Agent Orchestration\n",
    "from langgraph.graph import END, MessageGraph, StateGraph\n",
    "\n",
    "### Model APIs\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from openai import OpenAI\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing PDF parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "### PDF Parsing\n",
    "import PyPDF2\n",
    "from tabula.io import convert_into, read_pdf\n",
    "\n",
    "\n",
    "# Choose pdf file\n",
    "pdf_dir = \"data/pdfs\"\n",
    "pdf_files = os.listdir(pdf_dir)\n",
    "pdf_file = f\"{pdf_dir}{pdf_files[3]}\"\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "pdf_len = len(pdf_reader.pages)\n",
    "print(f\"{pdf_file = }\")\n",
    "\n",
    "\n",
    "# Parse pdf file\n",
    "\n",
    "pdf_table_index = []\n",
    "for page_num in range(1, pdf_len + 1):\n",
    "    tables = read_pdf(pdf_file, pages=str(page_num))\n",
    "    for idx, table in enumerate(tables):\n",
    "        nu_dict = {}\n",
    "        nu_dict = {\n",
    "            \"page_num\": int(page_num),\n",
    "            \"table_num\": int(idx),\n",
    "            # \"table_dataframe\":table,\n",
    "            \"table_dict\": table.to_dict(),\n",
    "        }\n",
    "\n",
    "        pdf_table_index.append(nu_dict)\n",
    "    del tables\n",
    "\n",
    "_ = [print(x) for x in pdf_table_index]\n",
    "\n",
    "# print(pdf_table_index[0])\n",
    "# output = convert_into(pdf_file, \"output.csv\", output_format=\"csv\", pages='all')\n",
    "\n",
    "# loader = PDFMinerLoader(pdf_file)\n",
    "# # loader = PyMuPDFLoader(pdf_file)\n",
    "# # loader = PDFPlumberLoader(pdf_file)\n",
    "\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from libs.env import *\n",
    "\n",
    "from libs.agents import Agent, AgentConfig, create_agent, invoke_agent\n",
    "from libs.base import Directories\n",
    "from libs.graphs import run_team_workflow\n",
    "from libs.io import read_text, write_text,read_file,write_file\n",
    "# from libs.apis import APIS\n",
    "\n",
    "import chromadb\n",
    "# from chromadb import Client as VectorDBClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader,PyMuPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "import pymupdf\n",
    "import fitz \n",
    "from time import sleep\n",
    "\n",
    "\n",
    "dirs = Directories()\n",
    "\n",
    "log_file_path = os.path.join(dirs.logs,'log.py')\n",
    "\n",
    "# Need a BIG data repository...\n",
    "dir_filestore = \"data/pdfs\"\n",
    "dir_vector_db = \"data/databases\"\n",
    "\n",
    "\n",
    "\n",
    "# print_heading(\"Available APIs and Models\",'green')\n",
    "# eprint(MODELS)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print_heading(\"Available Agent Personas\",'green')\n",
    "eprint([x for x in list(AGENTS.keys())])\n",
    "print(\"\\n\")\n",
    "\n",
    "print_heading(\"Project Directories\",'green')\n",
    "print_dict(dirs.__dict__,'green')\n",
    "\n",
    "# forked from urobot\n",
    "import re\n",
    "\n",
    "import camelot\n",
    "import fitz\n",
    "import pandas as pd\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextBoxHorizontal, LTTextContainer\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "\n",
    "def aggregate_table_to_text(series, separator=\"; \"):\n",
    "    aggregated_text = separator.join(series.astype(str))\n",
    "\n",
    "    return aggregated_text\n",
    "\n",
    "\n",
    "def prepare_all_text_data(df, separator=\" | \", clean=True):\n",
    "    combined_text = df.apply(lambda row: separator.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    if clean:\n",
    "        # Define a cleaning function\n",
    "        def clean_text(text):\n",
    "            text = text.lower()  # Lowercase text\n",
    "            text = re.sub(r\"<.*?>+\", \"\", text)  # Remove HTML tags\n",
    "            text = re.sub(r\"[\\r|\\n|\\r\\n]+\", \" \", text)  # Remove line breaks\n",
    "            text = re.sub(r\"[\\W_]+\", \" \", text)  # Remove punctuation\n",
    "            text = re.sub(\n",
    "                r\"\\s+\", \" \", text\n",
    "            )  # Replace multiple spaces with a single space\n",
    "            return text.strip()\n",
    "\n",
    "        # Apply cleaning function\n",
    "        cleaned_text = combined_text.apply(clean_text)\n",
    "    else:\n",
    "        cleaned_text = combined_text\n",
    "\n",
    "    # Handle missing values\n",
    "    cleaned_text = cleaned_text.fillna(\"\")\n",
    "\n",
    "    # Add the prepared text to the DataFrame\n",
    "    df[\"prepared_text\"] = cleaned_text\n",
    "\n",
    "    df = df[\"prepared_text\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_text_with_page_numbers(pdf_path, config):\n",
    "    pages = None\n",
    "    if config[\"filter_toc_refs\"]:\n",
    "        start_page, end_page = get_relevant_pages(pdf_path)\n",
    "        pages = list(range(start_page, end_page + 1))\n",
    "\n",
    "    filtered = []\n",
    "    pages_per_chunk = []  # List to store lists of page numbers for each text chunk\n",
    "    temp_string = \"\"  # Temporary string to accumulate text\n",
    "    temp_pages = []  # Temporary list to track pages for the current text chunk\n",
    "    current_page = 1  # Start from the first page\n",
    "\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        if page_layout.pageid in pages or pages is None:\n",
    "            for element in page_layout:\n",
    "                if isinstance(element, LTTextContainer):\n",
    "                    text = element.get_text()\n",
    "                    splitted = text.split(\"\\n\\n\")  # Split text into paragraphs\n",
    "\n",
    "                    for string in splitted:\n",
    "                        cleaned_string = \" \".join(string.replace(\"\\n\", \" \").split())\n",
    "                        if (\n",
    "                            re.compile(r\"\\bREFERENCES\\b\").search(cleaned_string)\n",
    "                            and pages is not None\n",
    "                        ):\n",
    "                            return filtered, pages_per_chunk\n",
    "\n",
    "                        if (\n",
    "                            cleaned_string\n",
    "                        ):  # Check if there is actual content after cleaning\n",
    "                            if current_page not in temp_pages:\n",
    "                                temp_pages.append(\n",
    "                                    current_page\n",
    "                                )  # Add the current page number if not already included\n",
    "\n",
    "                            temp_string += (\n",
    "                                cleaned_string + \" \"\n",
    "                            )  # Accumulate cleaned text\n",
    "\n",
    "                            # Check conditions to finalize the current chunk\n",
    "                            if len(temp_string) > config[\"chunk_threshold\"] and (\n",
    "                                temp_string.endswith(\". \")\n",
    "                                or temp_string.endswith(\"? \")\n",
    "                                or temp_string.endswith(\"! \")\n",
    "                            ):\n",
    "                                filtered.append(temp_string.strip())\n",
    "                                pages_per_chunk.append(temp_pages.copy())\n",
    "                                temp_string = \"\"  # Reset temporary string\n",
    "                                temp_pages = (\n",
    "                                    []\n",
    "                                )  # Reset page tracking for the next chunk\n",
    "\n",
    "        current_page += 1  # Move to the next page\n",
    "\n",
    "    # Handle any remaining text chunk after the last page\n",
    "    if temp_string.strip():\n",
    "        filtered.append(temp_string.strip())\n",
    "        pages_per_chunk.append(temp_pages)\n",
    "\n",
    "    return filtered, pages_per_chunk\n",
    "\n",
    "\n",
    "def extract_by_char_limit(pdf_path, threshold=500):\n",
    "    text = extract_text(pdf_path)\n",
    "    splitted = text.split(\"\\n\\n\")  # Initial split by double new lines to get paragraphs\n",
    "\n",
    "    filtered = []\n",
    "    temp_string = \"\"  # Temporary string to accumulate text\n",
    "\n",
    "    for string in splitted:\n",
    "        # Remove consecutive new lines within a paragraph, and trim multiple spaces\n",
    "        cleaned_string = \" \".join(string.replace(\"\\n\", \" \").split())\n",
    "\n",
    "        # Proceed with accumulation and checking against the threshold\n",
    "        temp_string += cleaned_string + \" \"  # Add space for separation\n",
    "\n",
    "        # Check if the accumulated string meets criteria to be added to filtered\n",
    "        if len(temp_string) > threshold and (\n",
    "            temp_string.endswith(\". \")\n",
    "            or temp_string.endswith(\"? \")\n",
    "            or temp_string.endswith(\"! \")\n",
    "        ):\n",
    "            filtered.append(\n",
    "                temp_string.strip()\n",
    "            )  # Append to filtered and remove trailing space\n",
    "            temp_string = \"\"  # Reset temporary string\n",
    "        elif len(cleaned_string) > threshold:\n",
    "            # If there's significant content in temp_string, add it first\n",
    "            if len(temp_string.strip()) > len(cleaned_string):\n",
    "                filtered.append(temp_string.strip())\n",
    "                temp_string = \"\"  # Reset for next accumulation\n",
    "            else:\n",
    "                # If temp_string was mostly the current string, start fresh\n",
    "                temp_string = (\n",
    "                    cleaned_string + \" \"\n",
    "                )  # Start accumulation afresh with current string\n",
    "\n",
    "    # Make sure to add any remaining accumulated text\n",
    "    if temp_string.strip():\n",
    "        filtered.append(temp_string.strip())\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def extract_by_paragraphs(pdf_path):\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    last_y0 = None\n",
    "\n",
    "    # Set up the PDF page aggregator\n",
    "    laparams = LAParams()\n",
    "    resource_manager = PDFResourceManager()\n",
    "    device = PDFPageAggregator(resource_manager, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "\n",
    "    for page in PDFPage.get_pages(open(pdf_path, \"rb\")):\n",
    "        interpreter.process_page(page)\n",
    "        layout = device.get_result()\n",
    "        for element in layout:\n",
    "            if isinstance(element, LTTextBox):\n",
    "                for text_line in element:\n",
    "                    # Check the y0 position to determine if this line is part of a new paragraph\n",
    "                    if last_y0 is not None and (last_y0 - text_line.y0) > 100:\n",
    "                        # Consider as new paragraph if the gap is big enough\n",
    "                        paragraphs.append(current_paragraph.strip())\n",
    "                        current_paragraph = text_line.get_text()\n",
    "                    else:\n",
    "                        current_paragraph += \" \" + text_line.get_text()\n",
    "                    last_y0 = text_line.y0\n",
    "    if current_paragraph.strip() != \"\":\n",
    "        paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def extract_tables(pdf_path):\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"lattice\")\n",
    "\n",
    "    tables_list = []\n",
    "    table_pages = []\n",
    "\n",
    "    # Iterate through tables and print them\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        tables_list.append(table.df)\n",
    "        table_pages.append(table.page)\n",
    "\n",
    "    return tables_list, table_pages\n",
    "\n",
    "\n",
    "def find_captions_with_locations(pdf_path):\n",
    "    captions = []\n",
    "    potential_blocks = []\n",
    "\n",
    "    # Step 1: Broadly identify potential caption blocks\n",
    "    for page_layout in extract_pages(pdf_path, laparams=LAParams()):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextBoxHorizontal):\n",
    "                text = element.get_text()\n",
    "                # Look for the presence of key phrases, numbering patterns, or table captions including the new generalized pattern\n",
    "                if (\n",
    "                    re.search(\n",
    "                        r\"\\d+(\\.\\d+)*\\s+Summary of evidence and\", text, re.IGNORECASE\n",
    "                    )\n",
    "                    or \"Summary of evidence and\" in text\n",
    "                    or re.search(r\"\\d+\\.\\d+\\.\\d+(\\.\\d+)?\", text)\n",
    "                    or re.match(r\"Table\\s+\\d+\\.\\d+\", text)\n",
    "                    or re.match(r\"Table\\s+\\d+:\", text)\n",
    "                ):\n",
    "                    potential_blocks.append((text, element.y0, page_layout.pageid))\n",
    "\n",
    "    # Step 2: Refine and extract captions from potential blocks\n",
    "    for block, y0, pageid in potential_blocks:\n",
    "        # Split the block into lines for more granular processing\n",
    "        lines = block.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            # Check each line for the target patterns, including the new generalized pattern\n",
    "            if (\n",
    "                re.search(r\"\\d+(\\.\\d+)*\\s+Summary of evidence and\", line, re.IGNORECASE)\n",
    "                or \"Summary of evidence and guidelines\" in line\n",
    "                or re.match(r\"Table\\s+\\d+\\.\\d+\", line)\n",
    "                or re.match(r\"Table\\s+\\d+:\", line)\n",
    "            ):\n",
    "                caption = line.strip()  # Clean up the line to serve as the caption\n",
    "                captions.append((caption, y0, pageid))\n",
    "                break  # Assuming one primary caption per block; adjust if needed\n",
    "\n",
    "    return captions\n",
    "\n",
    "\n",
    "def associate_captions_with_tables(captions, tables):\n",
    "    caption_table_pairs = []\n",
    "\n",
    "    for table in tables:\n",
    "        page_number = table.page\n",
    "        table_top = table._bbox[3]  # Top coordinate of the table\n",
    "        page_captions = {}\n",
    "\n",
    "        i = 0\n",
    "        for cap in captions:\n",
    "            if cap[2] == page_number:\n",
    "                page_captions.update({i: {\"cap\": cap[0], \"dist\": cap[1]}})\n",
    "                i += 1\n",
    "            elif cap[2] == page_number - 1:\n",
    "                page_captions.update({i: {\"cap\": cap[0], \"dist\": cap[1] + 420}})\n",
    "                i += 1\n",
    "\n",
    "        closest_caption = None\n",
    "        min_distance = float(\"inf\")\n",
    "\n",
    "        for caption in page_captions:\n",
    "            distance = abs(page_captions[caption][\"dist\"] - table_top)\n",
    "            if distance < min_distance:\n",
    "                closest_caption = page_captions[caption]\n",
    "                min_distance = distance\n",
    "\n",
    "        if closest_caption:\n",
    "            caption_table_pairs.append((closest_caption[\"cap\"], table.df))\n",
    "\n",
    "    return caption_table_pairs\n",
    "\n",
    "\n",
    "def merge_dataframes_with_same_caption(list_of_tuples):\n",
    "    merged_dict = {}\n",
    "    # Iterate through the list of tuples\n",
    "    for caption, df in list_of_tuples:\n",
    "        # If the caption is already in the dictionary, concatenate the current dataframe with the existing one\n",
    "        if caption in merged_dict:\n",
    "            merged_dict[caption] = pd.concat(\n",
    "                [merged_dict[caption], df], ignore_index=True\n",
    "            )\n",
    "        else:\n",
    "            merged_dict[caption] = df\n",
    "\n",
    "    # Convert the dictionary back to a list of tuples\n",
    "    merged_list_of_tuples = [(caption, df) for caption, df in merged_dict.items()]\n",
    "\n",
    "    return merged_list_of_tuples\n",
    "\n",
    "\n",
    "def add_captions_as_rows(list_of_tuples):\n",
    "    result_list = []\n",
    "\n",
    "    for caption, df in list_of_tuples:\n",
    "        # Create a new dataframe with the caption row\n",
    "        if caption is not None:\n",
    "            caption_df = pd.DataFrame([caption], columns=[df.columns[0]])\n",
    "            # Fill remaining columns with empty strings\n",
    "            for col in df.columns[1:]:\n",
    "                caption_df[col] = \"\"\n",
    "\n",
    "            # Concatenate the caption dataframe with the original dataframe\n",
    "            # Reset index to avoid index duplication\n",
    "            new_df = pd.concat([caption_df, df], ignore_index=True)\n",
    "            result_list.append(new_df)\n",
    "        else:\n",
    "            result_list.append(df)\n",
    "\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def find_nearest_caption(page, table_top, last_caption):\n",
    "    pattern = r\"\\d+(\\.\\d+)+\\s+[A-Za-z]+.*\"\n",
    "    min_distance = float(\"inf\")\n",
    "    nearest_caption = \"\"\n",
    "\n",
    "    for block in page.get_text(\"blocks\"):\n",
    "        block_text = block[4].strip()\n",
    "        if re.match(pattern, block_text):\n",
    "            block_bottom = block[3]\n",
    "            distance = table_top - block_bottom\n",
    "            if 0 < distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_caption = block_text\n",
    "\n",
    "    return nearest_caption if nearest_caption else last_caption\n",
    "\n",
    "\n",
    "def extract_and_filter_tables_with_captions(\n",
    "    pdf_path, tables, headings=[\"Summary of evidence\", \"Recommendations\"]\n",
    "):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    filtered_tables_with_captions = []\n",
    "    last_caption = None  # Initialize last_caption as None\n",
    "\n",
    "    for table in tables:\n",
    "        if any(heading in table.df.iloc[0, 0] for heading in headings):\n",
    "            page_num = table.page - 1\n",
    "            page = doc.load_page(page_num)\n",
    "            table_top_edge = table._bbox[1]\n",
    "\n",
    "            caption = find_nearest_caption(page, table_top_edge, last_caption)\n",
    "            last_caption = (\n",
    "                caption  # Update last_caption with the current caption for future use\n",
    "            )\n",
    "\n",
    "            filtered_tables_with_captions.append((caption, table.df))\n",
    "\n",
    "    return filtered_tables_with_captions\n",
    "\n",
    "\n",
    "def dataframe_to_markdown(df):\n",
    "    # Check if the first row can be used as the header or if it's a description\n",
    "    if df.shape[1] > 1:\n",
    "        if (pd.isna(df.iloc[0, 1]) or df.iloc[0, 1] == \"\") and df.iloc[0, 0]:\n",
    "            description = df.iloc[\n",
    "                0, 0\n",
    "            ].strip()  # Store the description, removing any leading/trailing whitespace\n",
    "            df = df.drop(0).reset_index(drop=True)  # Remove the description row\n",
    "        else:\n",
    "            description = None\n",
    "    else:\n",
    "        description = df.iloc[\n",
    "            0\n",
    "        ]  # Store the description, removing any leading/trailing whitespace\n",
    "        df = df.drop(0).reset_index(drop=True)  # Remove the description row\n",
    "\n",
    "    # Set the first row with entries as headers\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0).reset_index(drop=True)\n",
    "\n",
    "    # Convert DataFrame to Markdown\n",
    "    markdown_table = df.to_markdown(index=False)\n",
    "\n",
    "    # Prepend description if it exists\n",
    "    if description is not None:\n",
    "        markdown_table = f\"{description}\\n\\n{markdown_table}\"\n",
    "\n",
    "    return markdown_table\n",
    "\n",
    "\n",
    "def get_relevant_pages(pdf_path):\n",
    "    # Refined regex patterns for specific patterns\n",
    "    intro_pattern = re.compile(r\"\\sINTRODUCTION\\b\")\n",
    "    refs_pattern = re.compile(r\"\\sREFERENCES\\b\")\n",
    "\n",
    "    intro_count = 0\n",
    "    refs_count = 0\n",
    "    start_page = None\n",
    "    end_page = None\n",
    "\n",
    "    # Initialize variables to store text of each page\n",
    "    text_pages = extract_text(pdf_path).split(\"\\f\")\n",
    "\n",
    "    # Iterate through each page's text\n",
    "    for i, page_text in enumerate(text_pages):\n",
    "        # Check for the second occurrence of the Introduction pattern\n",
    "        if intro_pattern.search(page_text):\n",
    "            intro_count += 1\n",
    "            if intro_count == 2:\n",
    "                start_page = i + 1  # We use i+1 since indices are zero-based\n",
    "\n",
    "        # Check for the second occurrence of the References pattern\n",
    "        if refs_pattern.search(page_text):\n",
    "            refs_count += 1\n",
    "            if refs_count == 2:\n",
    "                end_page = i\n",
    "                break  # No need to continue if we found both\n",
    "\n",
    "    # Return the range of pages between Introduction and References\n",
    "    if start_page is not None and end_page is not None:\n",
    "        return start_page, end_page + 1\n",
    "    else:\n",
    "        raise ValueError(\"Couldn't find the specified sections twice in the document.\")\n",
    "\n",
    "\n",
    "def extract_tables_and_captions_with_pdfminer(pdf_path, config):\n",
    "    captions = find_captions_with_locations(pdf_path)\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"lattice\")\n",
    "\n",
    "    caption_table_pairs = associate_captions_with_tables(captions, tables)\n",
    "    merged = merge_dataframes_with_same_caption(caption_table_pairs)\n",
    "    captioned_tables = add_captions_as_rows(merged)\n",
    "\n",
    "    evidence_recommendations_tables = extract_and_filter_tables_with_captions(\n",
    "        pdf_path, tables\n",
    "    )\n",
    "    evidence_recommendations_tables = add_captions_as_rows(\n",
    "        evidence_recommendations_tables\n",
    "    )\n",
    "\n",
    "    cleaned_tables = []\n",
    "    table_text = []\n",
    "    dfs = []\n",
    "    if config[\"markdown_tables\"]:\n",
    "        evidence_recommendations_tables = [\n",
    "            (dataframe_to_markdown(df), df) for df in evidence_recommendations_tables\n",
    "        ]\n",
    "        captioned_tables_md = [\n",
    "            (dataframe_to_markdown(df), df) for df in captioned_tables\n",
    "        ]\n",
    "\n",
    "        table_text += [t[0] for t in evidence_recommendations_tables]\n",
    "        dfs += [t[1] for t in evidence_recommendations_tables]\n",
    "\n",
    "        table_text += [t[0] for t in captioned_tables_md]\n",
    "        dfs += [t[1] for t in captioned_tables_md]\n",
    "\n",
    "    else:\n",
    "        for df in evidence_recommendations_tables:\n",
    "            cleaned_tables.append(\n",
    "                prepare_all_text_data(df, separator=config[\"separator\"], clean=False)\n",
    "            )\n",
    "\n",
    "        for df in captioned_tables:\n",
    "            cleaned_tables.append(\n",
    "                prepare_all_text_data(df, separator=config[\"separator\"], clean=False)\n",
    "            )\n",
    "        for table in cleaned_tables:\n",
    "            table_text.append(aggregate_table_to_text(table))\n",
    "\n",
    "    return table_text, dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_tables(pdf_path):\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"lattice\")\n",
    "\n",
    "    tables_list = []\n",
    "    table_pages = []\n",
    "\n",
    "    # Iterate through tables and print them\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        tables_list.append(table.df)\n",
    "        table_pages.append(table.page)\n",
    "\n",
    "    return tables_list, table_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'data/pdfs/table_00.pdf'\n",
    "\n",
    "table_list,table_pages = extract_tables(pdf_path)\n",
    "print(f\"{table_list = }\")\n",
    "print(f\"{table_pages = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_filepath = knowledge_dir_files[1]\n",
    "print(f\"{document_filepath = }\")\n",
    "\n",
    "if '.pdf' in document_filepath:\n",
    "    document_loader = PyMuPDFLoader(document_filepath)\n",
    "    pages = document_loader.load()\n",
    "\n",
    "pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymupdf\n",
    "import fitz \n",
    "from time import sleep\n",
    "\n",
    "document_filepath = knowledge_dir_files[1]\n",
    "print(f\"{document_filepath = }\")\n",
    "\n",
    "if '.pdf' in document_filepath:\n",
    "    document_loader = PyMuPDFLoader(document_filepath)\n",
    "    pages = document_loader.load()\n",
    "\n",
    "pages\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "if not hasattr(fitz.Page, \"find_tables\"):\n",
    "    raise RuntimeError(\"This PyMuPDF version does not support the table feature\")\n",
    "\n",
    "for document_filepath in knowledge_dir_files:\n",
    "    doc = fitz.open(document_filepath)\n",
    "    # page = doc[14]\n",
    "\n",
    "    for idx,page in enumerate(doc):\n",
    "        tabs = page.find_tables()  # detect the tables    \n",
    "        md_text = None\n",
    "        for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "            print(f\"file: {document_filepath} \\npage: {idx} \\ntable:{i}\")\n",
    "            md_text = tab.to_markdown()\n",
    "            Markdown(md_text)\n",
    "            sleep(1)\n",
    "\n",
    "            # cur_df = tab.to_pandas()\n",
    "            # display(cur_df)\n",
    "            # for cell in tab.header.cells:\n",
    "            #     page.draw_rect(cell,color=fitz.pdfcolor[\"red\"],width=0.3)\n",
    "            # page.draw_rect(tab.bbox,color=fitz.pdfcolor[\"green\"])\n",
    "            # print(f\"Table {i} column names: \\n{tab.header.names}, \\nexternal: {tab.header.external}\")\n",
    "        \n",
    "        # show_image(page, f\"Table & Header BBoxes\")\n",
    "\n",
    "tab.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "if not hasattr(fitz.Page, \"find_tables\"):\n",
    "    raise RuntimeError(\"This PyMuPDF version does not support the table feature\")\n",
    "\n",
    "for document_filepath in knowledge_dir_files:\n",
    "    doc = fitz.open(document_filepath)\n",
    "    # page = doc[14]\n",
    "\n",
    "    for idx,page in enumerate(doc):\n",
    "        tabs = page.find_tables()  # detect the tables    \n",
    "        md_text = None\n",
    "        for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "            print(f\"file: {document_filepath} \\npage: {idx} \\ntable:{i}\")\n",
    "            md_text = tab.to_markdown()\n",
    "            Markdown(md_text)\n",
    "            sleep(1)\n",
    "\n",
    "            # cur_df = tab.to_pandas()\n",
    "            # display(cur_df)\n",
    "            # for cell in tab.header.cells:\n",
    "            #     page.draw_rect(cell,color=fitz.pdfcolor[\"red\"],width=0.3)\n",
    "            # page.draw_rect(tab.bbox,color=fitz.pdfcolor[\"green\"])\n",
    "            # print(f\"Table {i} column names: \\n{tab.header.names}, \\nexternal: {tab.header.external}\")\n",
    "        \n",
    "        # show_image(page, f\"Table & Header BBoxes\")\n",
    "\n",
    "tab.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "fname = 'code/pdfs/partition_example.pdf'\n",
    "\n",
    "elements = partition_pdf(filename=fname,\n",
    "                         infer_table_structure=True,\n",
    "                         strategy='hi_res',\n",
    "           )\n",
    "\n",
    "\n",
    "tables = [el for el in elements if el.category == \"Table\"]\n",
    "\n",
    "print(tables[0].text)\n",
    "print(tables[0].metadata.text_as_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- The existing method does a good job of extracting the text from the pdf document  \n",
    "- It seems that some of the tabular details are lost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "### Testing SQLite\n",
    "\n",
    "sqlite_path = \"M:/Code/ML/ELH/data/databases/sql/agent_db.sqlite\"\n",
    "\n",
    "### Create sqlite3 database\n",
    "sqlite_db = ail.create_sqlite_connection(sqlite_path)\n",
    "\n",
    "### Create users table\n",
    "ail.execute_query(\n",
    "    sqlite_db,\n",
    "    (\n",
    "        \"\"\"CREATE TABLE IF NOT EXISTS users (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT NOT NULL,\n",
    "        age INTEGER,\n",
    "        gender TEXT,\n",
    "        nationality TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    ),\n",
    "\n",
    ")\n",
    "\n",
    "# Load table\n",
    "table_name = 'users'\n",
    "users_table = pd.read_sql(f\"select * from {table_name}\", sqlite_db)\n",
    "users_table\n",
    "\n",
    "# Add record\n",
    "# cols = [\"name\",\"age\",\"gender\",\"nationality\"]\n",
    "# vals = [\"Me\",\"35\",\"male\",\"american\"]\n",
    "# cur_query = f\"INSERT INTO users {cols} VALUES {vals}\"\n",
    "\n",
    "### EH CUSTOM\n",
    "# # ail.execute_query(sqlite_db, cur_query)\n",
    "\n",
    "### PANDAS\n",
    "# pd.read_sql_query(cur_query, sqlite_db)\n",
    "\n",
    "### PYSPARK\n",
    "# pyspark.sql(ail.execute_query(sqlite_db,\"SELECT * FROM users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    I am creating a logo for XYZ.    \n",
    "    \"\"\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=prompt,\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    "    quality=\"standard\",\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo of Tools\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_name, messages=messages, tools=tools, tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(completion)\n",
    "\n",
    "chat_completion = {\n",
    "    \"id\": \"chatcmpl-123\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1677652288,\n",
    "    \"model\": \"gpt-3.5-turbo-0125\",\n",
    "    \"system_fingerprint\": \"fp_44709d6fcb\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
    "            },\n",
    "            \"logprobs\": None,\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 9, \"completion_tokens\": 12, \"total_tokens\": 21},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WEB SCRAPING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Can you give me a brief tutorial of python package langgraph? \n",
    "    Please reference the official documentation found at: \n",
    "    https://langchain-ai.github.io/langgraph/how-tos/docs/quickstart/\n",
    "    \"\"\"\n",
    "\n",
    "response_template = \"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Answer: Let's think step by step. \n",
    "    Please wrap your answer in lines that are maximally 100 characters in length.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = [\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/documents.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/text.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/sections.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/hdrftr.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/styles-understanding.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/styles-using.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/user/shapes.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/document.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/settings.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/style.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/settings.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/style.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/text.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/table.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/section.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/shape.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/dml.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/shared.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/enum/index.html\",\n",
    "    \"https://python-docx.readthedocs.io/en/latest/api/enum/MsoColorType.html\",\n",
    "]\n",
    "\n",
    "\n",
    "website_text = []\n",
    "\n",
    "for site in website_url:\n",
    "    website_text.append(scrape_website(site))\n",
    "\n",
    "all_text = \"\".join(website_text)\n",
    "print(all_text)\n",
    "\n",
    "with open(\n",
    "    root_path + \"python_docx_documentation_\" + cur_date + \".txt\", mode=\"w\"\n",
    ") as file:\n",
    "    file.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    website_url = (\n",
    "        \"https://example.com\"  # Replace with the URL of the website you want to scrape\n",
    "    )\n",
    "\n",
    "    # Scrape text from the website\n",
    "    website_text = scrape_website(website_url)\n",
    "\n",
    "    if website_text:\n",
    "        # Generate a response using GPT-3 based on the scraped text\n",
    "        prompt = f\"Read the following text from {website_url}: {website_text}\"\n",
    "        response = generate_response(prompt)\n",
    "\n",
    "        print(\"Generated Response:\")\n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"Failed to scrape text from the website.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
