{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m--------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[32m    Available Agent Personas\u001b[0m\n",
      "\u001b[1m\u001b[32m--------------------------------\u001b[0m\n",
      "['default',\n",
      " 'embedder',\n",
      " 'responder',\n",
      " 'goal_engineer',\n",
      " 'director',\n",
      " 'team_reporter',\n",
      " 'project_reporter',\n",
      " 'project_manager',\n",
      " 'task_manager',\n",
      " 'task_decomposer',\n",
      " 'prompt_engineer',\n",
      " 'source_selector',\n",
      " 'researcher',\n",
      " 'reviewer',\n",
      " 'user_proxy',\n",
      " 'developer']\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[32m---------------------------\u001b[0m\n",
      "\u001b[1m\u001b[32m    Project Directories\u001b[0m\n",
      "\u001b[1m\u001b[32m---------------------------\u001b[0m\n",
      "\u001b[1m\u001b[32mroot:     \u001b[0m \u001b[32mm:\\Code\\dawgpyl\u001b[0m\n",
      "\u001b[1m\u001b[32mlibs:     \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\libs\u001b[0m\n",
      "\u001b[1m\u001b[32msecurity: \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\security\u001b[0m\n",
      "\u001b[1m\u001b[32mconfigs:  \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\configs\u001b[0m\n",
      "\u001b[1m\u001b[32mdata:     \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\u001b[0m\n",
      "\u001b[1m\u001b[32mdocs:     \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\docs\u001b[0m\n",
      "\u001b[1m\u001b[32mdatabases:\u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\databases\u001b[0m\n",
      "\u001b[1m\u001b[32mmodels:   \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\models\u001b[0m\n",
      "\u001b[1m\u001b[32mlogs:     \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\logs\u001b[0m\n",
      "\u001b[1m\u001b[32minputs:   \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\inputs\u001b[0m\n",
      "\u001b[1m\u001b[32moutputs:  \u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\outputs\u001b[0m\n",
      "\u001b[1m\u001b[32mout_files:\u001b[0m \u001b[32mm:\\Code\\dawgpyl\\data\\outputs\\files\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from libs.env import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from configs.apis import APIS\n",
    "\n",
    "from libs.agents import Agent, AgentConfig, create_agent, invoke_agent\n",
    "from libs.base import Directories, Timestamp\n",
    "from libs.graphs import run_team_workflow\n",
    "from libs.io import read_text, write_text\n",
    "\n",
    "# from rich import print as richprint\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "# from flask.views import F\n",
    "\n",
    "dirs = Directories()\n",
    "log_prefix = \"log_\"\n",
    "filename = log_prefix + Timestamp().date + \".joblib\"\n",
    "log_file_path = os.path.join(dirs.logs, filename)\n",
    "\n",
    "# print_heading(\"Available APIs and Models\",'green')\n",
    "# eprint(MODELS)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print_heading(\"Available Agent Personas\", \"green\")\n",
    "eprint([x for x in list(AGENTS.keys())])\n",
    "print(\"\\n\")\n",
    "\n",
    "print_heading(\"Project Directories\", \"green\")\n",
    "print_dict(dirs.__dict__, \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ FAIL:\n",
      "m:\\Code\\dawgpyl\\data\\logs\\log_2024-11-13.joblib\n",
      "Error: [Errno 2] No such file or directory: 'm:\\\\Code\\\\dawgpyl\\\\data\\\\logs\\\\log_2024-11-13.joblib'\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Load daily response log and display last response\n",
    "\n",
    "try:\n",
    "    log = read_file(log_file_path)\n",
    "    try:\n",
    "        display(md(log[-1][\"response\"]))\n",
    "    except:\n",
    "        print(log)\n",
    "except:\n",
    "    print(\"DAILY LOG NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_context(filepaths: list | str):\n",
    "\n",
    "    if filepaths:\n",
    "        if isinstance(filepaths, list):\n",
    "            prompt_input = \"CONTEXT\"\n",
    "            for f in filepaths:\n",
    "                prompt_input += \"\\n\" + str(\n",
    "                    read_file(f)\n",
    "                )  # This should only be done files that are read as string (.txt, .py, etc.)\n",
    "\n",
    "        else:\n",
    "            prompt_input = read_file(filepaths)\n",
    "    else:\n",
    "        prompt_input = \"None\"\n",
    "\n",
    "    return prompt_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Wanting it all is an ambitious goal and achieving it will require clear focus and prioritization. Here are some steps that might help you:\n",
       "\n",
       "1. **Define What \"All\" Means**: Clarify what \"having it all\" entails for you personally. Consider different areas such as career, relationships, finances, health, and personal fulfillment.\n",
       "\n",
       "2. **Set Clear Goals**: Break down your vision into specific, achievable goals. Make sure they are SMART (Specific, Measurable, Achievable, Relevant, Time-bound).\n",
       "\n",
       "3. **Prioritize**: Determine which goals are most important and tackle them first.\n",
       "\n",
       "4. **Create a Plan**: Develop a strategy for achieving your goals. This may involve setting short- and long-term plans and milestones.\n",
       "\n",
       "5. **Manage Your Time**: Use effective time management techniques to maximize productivity and maintain a balance between different areas of your life.\n",
       "\n",
       "6. **Stay Flexible**: Life changes, and so do priorities and opportunities. Be willing to adapt your plan as needed.\n",
       "\n",
       "7. **Seek Support and Resources**: Surround yourself with a supportive network and seek resources that will help you on your journey.\n",
       "\n",
       "8. **Take Care of Yourself**: Ensure you have the energy and mental clarity to pursue your goals by maintaining your physical and emotional well-being.\n",
       "\n",
       "Remember that \"having it all\" is subjective and can change over time, so regular reflection and adjustment are essential."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRITE SUCCESS:\n",
      "m:\\Code\\dawgpyl\\data\\logs\\log_2024-11-13.joblib\n",
      "Response logged to: m:\\Code\\dawgpyl\\data\\logs\\log_2024-11-13.joblib\n"
     ]
    }
   ],
   "source": [
    "### General Q&A\n",
    "### Code Development\n",
    "save_response = True\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "max_tokens = None\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# prompt_files = [\"data/knowledge/file_00.md\"]\n",
    "\n",
    "# prompt_input = load_prompt_context(prompt_files)\n",
    "\n",
    "# prompt_user = f\"\"\"\n",
    "#     We are planning a family reunion in which we'll need to buy food and drinks for 50 adults and 20 kids for one week.\n",
    "#     Please make a shopping list for costco, broken down by category.\n",
    "#     Provide estimated costs (both total and per person)\n",
    "#     Present your results in a markdown table.\n",
    "# \"\"\"\n",
    "prompt_user = f\"\"\"\n",
    "    I want it all, how do I get it?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_system = \"\"\"\n",
    "            You are an expert executive assistant. \n",
    "            Your goal is to answer questions truthfully and succinctly, \n",
    "            providing clear and concise instructions when appropriate.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "chat_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt_system\": prompt_system,\n",
    "    \"prompt_user\": prompt_user,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "### Invoke Response\n",
    "response = chat(**chat_config)\n",
    "\n",
    "### Display Response\n",
    "try:\n",
    "    display(md(response.choices[0].message.content))\n",
    "except:\n",
    "    try:\n",
    "        print(response.choices[0].message.content)\n",
    "    except:\n",
    "        print(response)\n",
    "\n",
    "\n",
    "### Save Response\n",
    "if save_response:\n",
    "    log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General Q&A\n",
    "### Code Development\n",
    "save_response = True\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "# model_name = \"o1-preview\"\n",
    "max_tokens = None\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "prompt_files = []\n",
    "\n",
    "prompt_input = load_prompt_context(prompt_files)\n",
    "\n",
    "prompt_system = \"\"\"\n",
    "            You are an expert assistant.  \n",
    "            You answer questions trutfully and succinctly, providing citations for relevant facts.\"\"\"\n",
    "\n",
    "prompt_user = f\"\"\"\n",
    "    I would like you to discuss strategies for effective collaboration. After outlining these strategies, please draft an email directed to my colleagues in which you explain this way of working.  \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_config = {\n",
    "    \"model_name\":model_name,\n",
    "    \"prompt_system\":prompt_system,\n",
    "    \"prompt_user\":prompt_user,\n",
    "    \"max_tokens\":max_tokens,\n",
    "    }\n",
    "\n",
    "### Invoke Response\n",
    "response = chat(**chat_config)\n",
    "\n",
    "### Display Response\n",
    "try:\n",
    "    display(md(response.choices[0].message.content))\n",
    "except:\n",
    "    try:\n",
    "        print(response.choices[0].message.content)\n",
    "    except:\n",
    "        print(response)\n",
    "\n",
    "\n",
    "### Save Response\n",
    "if save_response:\n",
    "    log_response(response, chat_config, dirs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Task Breakdown\n",
    "\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "# prompt_system = \"\"\"\n",
    "# You are a skilled research assistant.\n",
    "# You provide succinct yet comprehensive responses to each user request.\n",
    "# You retrieve information from reliable and trustworthy sources. You cite these sources when appropriate.\n",
    "# Before responding, you consider the logical steps needed to provide your response and articulate them.\n",
    "# Your final response should be presented in markdown format.\n",
    "# \"\"\"\n",
    "# prompt_user = \"\"\"\n",
    "# I would like help breaking down complex tasks into more manageable steps. Please use the web to research methods for breaking down complex tasks into sub-tasks, then provide a summary of your findings.\n",
    "# \"\"\"\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Writing w/ Context Files\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "\n",
    "# ###############################################################################################################\n",
    "# ### Use this section if you want one file read for the system prompt and one file read for the user prompt\n",
    "# filepaths = [\n",
    "#     \"data/knowledge/file_00.md\",\n",
    "#     ]\n",
    "\n",
    "\n",
    "# prompt_input = load_prompt_files(filepaths[0])\n",
    "# prompt_system = f\"\"\"\n",
    "# You are a skilled scientific writer.\n",
    "# You excel at writing about complex topics with clarity and accuracy.\n",
    "# Your writing style is similar to other highly-cited researcher who publish in top academic journals (Nature, Science, New England Journal of Medicine, etc.)\n",
    "# You retrieve information from reliable and trustworthy sources. You cite these sources when appropriate.\n",
    "# Before responding, you consider the logical steps needed to provide your response and articulate them step-by-step.\n",
    "\n",
    "# The following information is the context for your writing assignment.\\n\n",
    "\n",
    "# CONTEXT:\\n\\n\n",
    "# {prompt_input}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# #Please review the following text and provide editorial guidance to improve the clarity and quality of the text.\n",
    "# #Please review the following text and provide 2-3 additional sentences to each section which expand and elaborate on the current topics.\n",
    "# prompt_input = load_prompt_files(\"data/document_00.txt\")\n",
    "\n",
    "# prompt_user = f\"\"\"\n",
    "# Please review the following text and edit sections to match the factual information contained in the CONTEXT. Additionally, use your editorial expertise to improve the scientific rigor, clarity, and quality of the text.\n",
    "\n",
    "# CURRENT_DRAFT:\\n\\n\n",
    "\n",
    "# {prompt_input}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code Documentation (inline)\n",
    "# save_response = True\n",
    "\n",
    "# model_name = \"o1-preview\"\n",
    "# max_tokens = None\n",
    "\n",
    "\n",
    "# ###############################################################################################################\n",
    "# filepath = \"code/code2clean_00.py\"\n",
    "# prompt_input = read_file(filepath)\n",
    "# prompt_system = \"\"\"You are an expert software engineer who writes detailed documentation\n",
    "#             for all of your code. Your preferred code documentation standard is the\n",
    "#             \"Google Python Style Guide\". Each argument of a function should contain\n",
    "#             a datatype within the definition line, and immediately under each\n",
    "#             function definition there should be a comment block describing the\n",
    "#             function's arguments and return values. If a function contains more than a few lines, you should\n",
    "#             also provide contextual comments inline with the code.\"\"\"\n",
    "\n",
    "# prompt_user = f\"\"\"\n",
    "\n",
    "#             Please review the following code and document it appropriately.\n",
    "\n",
    "#             'EXAMPLE CODE'\\n\\n\n",
    "#             {prompt_input}\\n\\n\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# chat_config = {\n",
    "#     \"model_name\":model_name,\n",
    "#     \"prompt_system\":prompt_system,\n",
    "#     \"prompt_user\":prompt_user,\n",
    "#     \"max_tokens\":max_tokens,\n",
    "#     }\n",
    "\n",
    "# ### Invoke Response\n",
    "# response = chat(**chat_config)\n",
    "\n",
    "# ### Display Response\n",
    "# try:\n",
    "#     display(md(response.choices[0].message.content))\n",
    "# except:\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ### Save Response\n",
    "# if save_response:\n",
    "#     log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Development\n",
    "save_response = True\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "max_tokens = None\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "prompt_files = []\n",
    "\n",
    "prompt_input = load_prompt_files(prompt_files)\n",
    "\n",
    "prompt_system = \"\"\"\n",
    "            You are an expert software engineer who writes efficient python code with \n",
    "            detailed docstrings and inline documentation for all of your code. \n",
    "            Your preferred code documentation standard is the \"Google Python Style Guide\".\n",
    "            Each argument of a function should contain a datatype within the definition line, \n",
    "            and immediately under each function definition there should be a comment block \n",
    "            describing the function's arguments and return values. \n",
    "            If a function contains more than a few lines, you should also provide contextual comments \n",
    "            inline with the code.\"\"\"\n",
    "\n",
    "prompt_user = f\"\"\"\n",
    "    I would like you to write a library of python functions that reads various filetypes, \n",
    "    describes their contents, and ensures that each file's contents satisfy a series of data quality checks. \n",
    "    \n",
    "    This library should be able to parse the following filetypes: ['.hdf5','.parquet','.avi','.pdf']\n",
    "\n",
    "    The descriptions of each files contents should include: ['file_size_in_megabytes','file_length','contains_datetimes','sample_rate']\n",
    "\n",
    "    The filetype-specific data quality checks should include aspects of the data such as: \n",
    "    ['readable','has_missing_values']\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt_system\": prompt_system,\n",
    "    \"prompt_user\": prompt_user,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "### Invoke Response\n",
    "response = chat(**chat_config)\n",
    "\n",
    "### Display Response\n",
    "try:\n",
    "    display(md(response.choices[0].message.content))\n",
    "except:\n",
    "    try:\n",
    "        print(response.choices[0].message.content)\n",
    "    except:\n",
    "        print(response)\n",
    "\n",
    "\n",
    "### Save Response\n",
    "if save_response:\n",
    "    log_response(response, chat_config, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it seems the conclusion is I need to be using HDF5 files, but joblib is WAAAAY easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_log_path = \"data/logs/LOG2EDIT.joblib\"\n",
    "edited_log = read_file(edited_log_path)\n",
    "\n",
    "### In case you need to manually edit a chat log\n",
    "# from openai.types.chat import ChatCompletion,ChatCompletionMessage\n",
    "# from openai.types.chat.chat_completion import Choice\n",
    "# from openai.types.completion_usage import CompletionUsage\n",
    "\n",
    "# write_file(edited_log, edited_log_path)\n",
    "# write_file(log, os.path.join(dirs.logs,'LOG2EDIT.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Simple Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS A SIMPLE OPENAI CHAT WRAPPER, USE IT TO EXAMINE RAW MODEL OUTPUT\n",
    "\n",
    "from libs.common import chat\n",
    "\n",
    "max_tokens = 2048\n",
    "model_name = \"gpt-4o\"\n",
    "prompt_system = \"You are a helpful assistant.\"\n",
    "prompt_user = \"Tell me a story about a fire department run by bunnies.\"\n",
    "\n",
    "response = chat(model_name, prompt_system, prompt_user, max_tokens)\n",
    "\n",
    "response_dict = response.__dict__\n",
    "print(response_dict[\"choices\"][0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
